The repository contains, analyzes the data for watchlists that were devised according to categorical similarities.  They serve as data points which are pooled together then analyzed. 
 The modeling technique encompasses ETL (Extract, Transform, Load) procedures which are simplified when using such software as SSRS (Sql Server Reporting and Sql Server Integration), and WIndows features in PowerBI, PowerPivot, Transact-Sql, and similar tools, Power Center (Informatica), Text to Columns (Microsoft Excel) wherein data is converted from a sql or database data type and integrated as a datatype for use with aggregations, sums, averages, standard mean, max, min and other commonly used deterministic statistics, probabilistic functions-useage.  The watchlists capture what is a compilation comprised of stocks, percent move (day-over-day), volume, date, 52-week high, 52-week low and other facts.  Such demographical information, or categorical information serves to analyze a watchlist by subcategory such as mid-cap, small-cap, NYSE or NASDAQ participation and others such as market (Tech, growth, value ,etc).
The repository further can be viewed through the lense of data modeling and big data.  These files were tested and run in a Big Data Platform (BDP- Cloudera Data Platform, Hadoop Distributed File System/HDFS, previously Hortonworks), integrate the open source (operating system (OS: centos, linux), whereby files can be imported, stored and used for storing data, script file storage, compilation or execution (run) without or with a console (command-line "executable" or console:  CLoudera/Ambari (hortonworks)).  The scripts which we use (python, scala, or java) capture hadoop, map reduce jobs.  The mapper and reducer functions pull (ETL) relevant columns, reduce or aggregate data by tuple, key/values and output metrics including, max, minimum, filters and slices, eg. which stocks had normal, exceptional (greater than 3 percent), or nominal (less than zero, less than one) histogram values.  In addition to this, a separate file-upload will be available for comparison within the pyspark (Spark (Apache) in a python setting) Big Data Platform (clusters).  Either methods use big data processing by parallelizing the work across a cluster (manager/master and worker/slave nodes) so that work is done in tandem allowing for the processing of much more work (Terabytes verses gigabites). 
 Data is contained in files stored simply as a file prefixed with "data" and stored as a semi-structured delimitted file (comma, tab or pipes); thus they appear as .csv files.  In addition, command line (Cli) executable (package) lines of code compiling or running a hadoop (map reduce job) using python, and they are saved as .txt files.  Thus, the repository will contain these and a py file (.py extension) (python) which is a script in which the Map Reduce Jobs library amd hadoop step classes are called.  In total there are three file types to use as discussed.
 A B S T R A C T -
Hundreds of stocks were initially selected, included for analysis using a cloud, the Cloudera Big Data Platform, (Hortonworks) which supports Hadoop, Hadoop FS (hdfs fs; file system), 
Ambari portal ( contains the cloud software including Hadoop, Files Manager/Viewer, Pig (alternative to Map Reduce/Python), Hive (sql-like tables use), Tez, also including Zookeeper (Apache's Data Catalog), yarn (manager of the worker node and node manager cluster).  Files are injested, transformed and loaded into the hadoop:  the files are cleaned for use in column stored data where the data type is transformed for use appropriately, cleaned by eliminating the zero's or empty data fields, and
enforcement of the transformation with ddl (data definition typing) by column/row for use with aggregation functions (sum, std. deviation- data science, eg.), remove columns/extrinsic data originally presented by the source, including commas, quotes and extra spaces in column names.  
Other files will show non Big Data modeling:  PowerBI and Excel are used to clean data before injest.  PowerBI and Excel offer the advantage of using native functions to specify delimtation, which splits data by delimtter, including, space, semicolon, tab, commas to capture the needed symbol from 
fields which often appear with a "long name'.  In other words ETL must often exclude data, eliminate the extra spacing or transform empty data into useable data ("zero" verses "-" (dashes)) from the Brokerage that encapsulate other data, "bid", "ask", etc; for the analysis of import, this is too much data; data stored for this big data useage (platform) is further distinguished by the tuple requirement, or,
a python container of data, or collection.  Python tuples allow up to five comma-delimitted fields as we know (eg., ("aapl" [ticker], $120.00 [price - numeric field type], 130.00 [max], etc).  *args, **kwargs were not used to expand the capacity to favor
tuple extension for use with more fields for analysis.  Moreover, the use of hadoop allows us to extract data columns individually.  This diverges from sql based database and database warehouses (oltp/OLAP) which
require retrieving and storing entire rows of data.  In other words were I interested in the price move of a stock over a period of time, for example, the enduser would query only that column field, the price move
which captures that field and is associated with the key for that value such as the symbol/ticker.  The term, kvp (key-value pair), like the no sql schema syntax, is a method (tuple) where the data is stored as pairs not as column and row.  For example the key value pair of aapl, 5%
is yielded from a table which shows stocks, stock symbol, price, price change, price change as percent, bid, ask, and date.  Big data clusters in other words process data that is
using multiple resources to simplify the data analytics without storing and retrieving and querying full records of data as a sql, row based file system or database would.
Files marked as .csv are cleaned files, files that were transformed for consistent data types, eg., number formats are used where the original record was provided as "general" or "string".
Calculations on "general" or "string" format require the casting to numeric, integer and real numbers for calculations including min,max, standard mean, std dev.  

The code which runs on the files is also contained and labeled as .py.  The python code is used to call hadoop map reduce jobs (mrjobs) that satisfies the requirement to query data not rows of
data.  The query for this study queries the data for a histogram.  Bins are marked as 5,4,3,2,1 ranking moves by percent.  a 5 rank is used to label the number of stocks that
made moves of 5 percent, for instance, or ranked higher than those which moved 4 (%). This study can be done at user discretion.  As investors we tend to analyze stocks on the
short, medium and long term which dates range from one week to several weeks to several months.  
